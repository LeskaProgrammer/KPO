import os
import re
import httpx
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from urllib.parse import quote

app = FastAPI(title="üîç File Analysis Service", version="1.0")


class AnalyzeRequest(BaseModel):
    submission_id: int
    file_path: str
    task_id: str
    student_name: str


FILE_SERVICE_URL = os.getenv("FILE_SERVICE_URL", "http://file-storing-service:8001")

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
STOP_WORDS = {
    # –†—É—Å—Å–∫–∏–µ
    '–∏', '–≤', '–Ω–∞', '–Ω–µ', '—á—Ç–æ', '—Å', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫',
    '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ',
    '–µ—ë', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â—ë', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É',
    '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '—É–∂–µ', '–¥–ª—è', '–≤–∞—Å', '–Ω–∏', '—Ä–∞–∑', '–µ—Å–ª–∏', '–∏–ª–∏', '—ç—Ç–æ',
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
    'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'it', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'we', 'they',
    # –ö–æ–¥
    'def', 'class', 'import', 'from', 'return', 'if', 'else', 'elif', 'for',
    'while', 'try', 'except', 'with', 'as', 'pass', 'break', 'continue', 'self'
}


def read_file_safely(file_path: str) -> tuple[str, bool]:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ —á–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Ç–µ–∫—Å—Ç, —É—Å–ø–µ—Ö)
    """
    encodings = ['utf-8', 'cp1251', 'latin-1', 'utf-16']

    for encoding in encodings:
        try:
            with open(file_path, "r", encoding=encoding) as f:
                return f.read(), True
        except (UnicodeDecodeError, UnicodeError):
            continue
        except FileNotFoundError:
            return "", False
        except Exception:
            continue

    # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
    try:
        with open(file_path, "rb") as f:
            content = f.read()
            # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            try:
                return content.decode('utf-8', errors='ignore'), True
            except:
                return "", False
    except:
        return "", False


@app.post("/analyze/")
async def analyze(req: AnalyzeRequest):
    """
    üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç.

    –õ–æ–≥–∏–∫–∞:
    - –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ > 80% —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–µ–π —Ä–∞–±–æ—Ç–æ–π –¥—Ä—É–≥–æ–≥–æ —Å—Ç—É–¥–µ–Ω—Ç–∞ ‚Üí PLAGIARISM DETECTED
    - –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ <= 80% ‚Üí Clean, –Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ
    - –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Ä–∞–±–æ—Ç–∞ ‚Üí Clean (No previous submissions)
    """

    # 1. –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª
    current_text, success = read_file_safely(req.file_path)

    if not success or not current_text:
        return {"verdict": "‚ö†Ô∏è Read Error:  Could not read file"}

    if len(current_text.strip()) < 10:
        return {"verdict": "‚úÖ Clean (Too short to analyze)"}

    # 2. –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏:  –°—Ç–∞—Ä—ã–µ -> –ù–æ–≤—ã–µ)
    history = []
    async with httpx.AsyncClient() as client:
        try:
            resp = await client.get(f"{FILE_SERVICE_URL}/history/{req.task_id}", timeout=10.0)
            if resp.status_code == 200:
                history = resp.json()
        except Exception as e:
            return {"verdict": f"‚ö†Ô∏è Service Error: {e}"}

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    documents = [current_text]
    valid_entries = []  # [{student, uploaded_at}, ...]

    for item in history:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º—É —Ä–∞–±–æ—Ç—É
        if str(item["id"]) == str(req.submission_id):
            continue
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ—Å–¥–∞—á–∏ —Ç–æ–≥–æ –∂–µ —Å—Ç—É–¥–µ–Ω—Ç–∞
        if item["student_name"] == req.student_name:
            continue

        text, success = read_file_safely(item["file_path"])
        if success and len(text.strip()) > 10:
            documents.append(text)
            valid_entries.append({
                "student": item["student_name"],
                "uploaded_at": item["uploaded_at"]
            })

    # 4. –ï—Å–ª–∏ –Ω–µ—Ç —Å —á–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å
    if len(documents) == 1:
        return {"verdict": "‚úÖ Clean (No previous submissions to compare)"}

    # 5. –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
    try:
        vec = TfidfVectorizer(
            min_df=1,
            stop_words=list(STOP_WORDS),
            ngram_range=(1, 3)  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ñ—Ä–∞–∑—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        )
        tfidf = vec.fit_transform(documents)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—Ç—É (–∏–Ω–¥–µ–∫—Å 0) —Å–æ –≤—Å–µ–º–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏
        cosine_sim = cosine_similarity(tfidf[0:1], tfidf[1:])

        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        max_index = cosine_sim[0].argmax()
        max_similarity = cosine_sim[0][max_index]
        max_percent = int(max_similarity * 100)
        closest_student = valid_entries[max_index]["student"]

        # –†–µ—à–µ–Ω–∏–µ –æ –ø–ª–∞–≥–∏–∞—Ç–µ
        if max_percent > 80:
            verdict = f"üö® PLAGIARISM DETECTED (Copied from {closest_student}, similarity: {max_percent}%)"
        elif max_percent > 50:
            verdict = f"‚ö†Ô∏è Suspicious (Similar to {closest_student}, similarity: {max_percent}%)"
        else:
            verdict = f"‚úÖ Clean (Closest:  {closest_student}, similarity: {max_percent}%)"

        return {"verdict": verdict, "similarity": max_percent, "compared_with": closest_student}

    except Exception as e:
        return {"verdict": f"‚ö†Ô∏è Analysis Error: {e}"}


class WordCloudRequest(BaseModel):
    file_path: str


@app.post("/wordcloud/")
async def generate_wordcloud(req: WordCloudRequest):
    """
    ‚òÅÔ∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–ª–∞–∫–æ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ QuickChart API.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
    - –¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    """

    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    text, success = read_file_safely(req.file_path)

    if not success:
        return {"error": "‚ùå Cannot read file"}

    if not text.strip():
        return {"error": "‚ùå File is empty"}

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ - –Ω–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞ (–±—É–∫–≤—ã –ª–∞—Ç–∏–Ω–∏—Ü—ã –∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã)
    words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())

    if not words:
        return {"error": "‚ùå No valid words found in file"}

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    words = [w for w in words if w not in STOP_WORDS]

    if not words:
        return {"error": "‚ùå Only stop-words found in file"}

    # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
    word_freq = Counter(words)
    top_words = word_freq.most_common(50)  # –¢–æ–ø-50 –¥–ª—è –æ–±–ª–∞–∫–∞

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è QuickChart
    # –§–æ—Ä–º–∞—Ç: "—Å–ª–æ–≤–æ: —á–∞—Å—Ç–æ—Ç–∞,—Å–ª–æ–≤–æ:—á–∞—Å—Ç–æ—Ç–∞,..."
    word_data = ",".join([f"{word}:{count}" for word, count in top_words])

    # URL-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏
    encoded_data = quote(word_data)

    # URL –¥–ª—è QuickChart Word Cloud API
    quickchart_url = f"https://quickchart.io/wordcloud?text={encoded_data}&width=800&height=400&fontScale=15&backgroundColor=white"

    # –¢–æ–ø-10 —Å —ç–º–æ–¥–∑–∏
    top_10_with_emoji = [
        {"word": word, "count": count, "rank": f"#{i + 1}"}
        for i, (word, count) in enumerate(word_freq.most_common(10))
    ]

    return {
        "wordcloud_url": quickchart_url,
        "top_10_words": top_10_with_emoji,
        "statistics": {
            "total_words": len(words),
            "unique_words": len(word_freq),
            "most_common_word": word_freq.most_common(1)[0] if word_freq else None
        }
    }


@app.get("/")
async def root():
    return {
        "service": "üîç File Analysis Service",
        "status": "‚úÖ running",
        "features": [
            "üìä Plagiarism detection (TF-IDF + Cosine Similarity)",
            "‚òÅÔ∏è Word cloud generation (QuickChart API)",
            "üî§ Stop-words filtering (RU/EN)"
        ]
    }